\documentclass{elsarticle}
\usepackage{amsfonts,xcolor}
\usepackage{amssymb}
\usepackage{amsmath}
\newcommand{\X}{\mathrm{X}}
\newcommand{\Xh}{\hat{\mathrm{X}}}
\newcommand{\transpose}{^\mathrm{T}}
\usepackage{geometry}
\usepackage{url}
\usepackage[boxed]{algorithm2e}
\usepackage{booktabs}
\usepackage{algorithmicx,algpseudocode}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\begin{document}
<<setup, include=FALSE, cache=FALSE,eval=FALSE>>=
options(xtable.comment = FALSE)
options(replace.assign=TRUE,width=90)
options( digits = 4 )
# knit_hooks$set(rgl = hook_rgl)
# head(hook_rgl)  # the hook function is defined as this
library(ANTsR)
library(boot)
library(xtable)
library(grid)
library(png)
library(abind)
library(grDevices)
library(infotheo)
library(xtable)
library(fastICA)
@

<<definevars, include=FALSE, cache=FALSE,eval=FALSE>>=
qv<-1;mysig<-1
@


\title{Eigenanatomy: Sparse Component Analysis for Medical Imaging}
\author{Brian B. Avants}
\begin{abstract}
Eigenanatomy is a general formulation for sparse component analysis of
medical imaging data.  Eigenanatomy (Eanat) may be used to reduce the
dimensionality of large datasets to a manageable size, extract
covarying patterns from structural or functional imaging or to
identify "network" predictors that may be used in standard population
analyses.  The Eanat framework implements a variety of possible sparse
decompositions where we may force components to be smooth along an
anatomical manifold.  Here, we summarize this new framework from a
theoretical and practical perspective.  Early results in a cortical
thickness study of Mild Cognitive Impairment (MCI) highlight
advantages with respect to voxel-wise methods.  We also contrast Eanat
results with PCA and fast-ICA.  The study software and data are
publicly available in a free framework, ANTsR, based on ANTs (image
registration, segmentation, bias correction) and R
(the statistical programming platform).
\end{abstract}
\maketitle
\section{Introduction}
High dimensional datasets are frequently collected in medicine and
biology.  Magnetic resonance imaging (MRI), gene expression and
genotype all contain thousands to millions of measurements per
individual.  The individual discrete measurements comprising these
modalities are related to each other through the lens of both the
quantitative technology and the underlying biology.  Therefore, the
resulting datasets often exhibit strong covariation and suffer from
the curse of dimensionality.  When this type of data is addressed with
univariate statistics, studies may be underpowered or fail to capture
the intrinsically multivariate nature of the underlying biological
signal.

Data-driven dimensionality reduction and feature selection techniques
provide a potentially optimal strategy for analyzing ``big data" in
biological domains \cite{}.  Dimensionality reduction methods find
(weighted) combinations of the univariate measurements such that the
original data is well-described by a relatively small set of summary
measurements.  Given the presence of collinearity, dimensionality
reduction methods may improve statistical power by collecting related
measurements together.  This also facilitates data inspection which is
challenging when using univariate approaches to data with several
thousand or more variables.

There are three commonly used methods for data driven dimensionality
reduction: 
\begin{itemize}
\item Principal Component Analysis (PCA)~\cite{jolliffe2005principal}: It is one of the most widely used
dimensionality reduction algorithms.  PCA, however, has the disadvantage that
the low-dimensional components consist of contributions from every
component of the high-dimensional space, which makes interpretation of
the low-dimensional space difficult.  Many techniques have been
proposed to deal with this issue.  One common method is Sparse PCA (SPCA)
\cite{jolliffe_simplified_2000,shen2008sparse,guan2009sparse}.  Sparse
PCA incorporates penalties on the matrix decomposition to encourage
each component to consist of contributions from only a few components
from the higher dimensional space. 
\item Non-negative Matrix Factorization
(NMF)~\cite{paatero1994positive}: It constrains the components to be
positive, which also often results in sparse components
\cite{lee_learning_1999}.  
\item Independent Component Analysis
(ICA)~\cite{hyvarinen2000independent}: It is motivated by
the ``blind source separation'' problem: Given a data matrix that
contains information from a variety of sources, how can we uncover the
original sources?  Because of the different motivation of ICA, its
notation is usually different from SPCA and NMF, making comparisons
between the various methods difficult.
\end{itemize}



Eigenanatomy is a general framework that is closely related to SPCA,
NMF, and a version of ICA.  The specific constraints implemented
within each of these methods alter the patterns that are extracted.
Each algorithm has a different history, employs different theoretical
arguments and is favored in a different community.  The optimization
methods are also heterogeneous, making it challenging to know, when
comparing different implementations of the algorithms, whether the
theoretical or practical differences are the root of performance
variation. To address this concern, we implement Eigenanatomy
decompositions with respect to a consistent data-term and then enforce
smoothness and sparsity constraints inspired by Sparse PCA (SPCA), sparse ICA
(SICA) and sparse NMF (SNMF).  This allows us to compare positively
constrained decompositions (as with NMF) to SICA and SPCA-like
decompositions.

Our main contributions in this paper are:

\begin{enumerate}
\item From a theoretical  standpoint, provide a unified objective
function for PCA, NMF and ICA. 
\item From a practical standpoint, provide a  general framework,
including uniform optimization algorithms, that allow for
easy comparison of the decompositions without the confounds of
different implementation strategies.
\item Provide novel hybrid methods that allow for incorporating different aspects of each kind of
decomposition into a single decomposition. 
\item Customization of Eanat for neuroimaging by using domain specific
sparsity and smoothness constraints which aid interpretability of
results and give superior predictive performance.
\item A thorough evaluation involving comparison with standard
approaches; and finally the public availability of our toolkit.
\end{enumerate}


The remaining paper is organized as follows; we first describe the various dimensionality reduction methods and provide a unified objective for them.
Then we describe our Eanat theoretical framework and detail how the
framework may be extended to implement neuro or medical imaging
specific decompositions.  Finally, we outline our optimization
algorithm and provide experimental evaluation.

\section{Methods}
In this section we describe the various matrix decomposition techniques and provide a unified objective for them.  Finally, we show how one can define hybrid matrix decomposition methods by using different aspects of the different approaches.



\subsection{Notation}
Define a $n\times p$ (rows by columns) matrix ${\bf X }$, where $n$ is the number of observations (subjects) and  $p$ is the number of total voxels.
We seek a sparse decomposition of the ${\bf X}$ matrix  into two matrices
$U$ and $V$, (such $X \approx UV^{\top}$), where $U \in \mathbb{R}^{n
\times k}$ is the loading matrix and $V \in \mathbb{R}^{p \times k}$
is the coefficient matrix. More generally, we seek an approximation $\hat{X}$ (for instance in the above case $\hat{X}= UV^{\top}$) to the data matrix matrix $X$,
\begin{equation}
\min \|X-\hat{X} \|^2_F
\end{equation}


 As mentioned earlier, there are a variety of ways to perform the
 above decomposition and create an approximation to $X$.
\subsection{Principal Component Analysis (PCA)}

The standard PCA objective function is 
\begin{equation}
\begin{aligned}
&\underset{U,V}{\operatorname{arg\,min}} & & \| X - UV^{\top} \|_2^2 \\
&\text{subject to} & &V^{\top} V = I_p.
\end{aligned}
\end{equation}
The above optimization can be solved easily by performing Singular
Value Decomposition (SVD) of the data covariance matrix ${\bf
X^{\top}X}$ or one can iteratively get the $U$ and $V$ via power iteration.

A potential problem with standard PCA is that the low-dimensional
components it finds consist of contributions from each of the $[$
components of the high-dimensional space, which can be undesirable in
a domain like brain imaging. 


Sparse PCA (SPCA) augments this objective function with $\ell_1$ sparsity
constraints on the columns of $V$. Unlike, standard PCA, SPCA does not normally include an explicit orthogonality constraint, as the reconstruction error term makes the orthogonality less important \cite{le_ica_2011}.
\begin{equation}
\begin{aligned}
&\underset{U,V}{\operatorname{arg\,min}} & & \| X - UV^{\top} \|_2^2 + \lambda \sum_i \|V_i\|_1
\end{aligned}
\end{equation}

\subsection{Non-negative Matrix Factorization (NMF)}

NMF requires both $U$ and $V$ to be non-negative.  
\begin{equation}
\begin{aligned}
&\underset{U, V}{\text{minimize}} &&\| \mathbf{X} - UV^{\top}\|_2^2 \\ 
& \text{subject to} & &\mathbf{U,V} \succeq 0,
\end{aligned}
\end{equation}
where $\succeq$ indicates element-wise inequality. This constraint fits naturally to problems in which the input data is non-negative, as is the case with images.  


\subsection{Independent Component Analysis (ICA)}
ICA comes from a different tradition than SPCA and NMF, and its notation is different.  The $X$ matrix in SPCA is usually transposed \cite{hyvarinen1999fast}, and $U$ and $V\transpose$ are known as $s$ and $A$ respectively.  (Because the $X$ is transposed, the ICA decomposition is normally written as $X = As$.).  ICA seeks to impose ``independence'' on the \textit{sources}, not the coefficient vectors, its constraints act on the $U$ matrix, not the $V$ matrix: 
\begin{equation}
\begin{aligned}
&\underset{U,V}{\operatorname{arg\,min}} & & \| X - UV^{\top} \|_2^2 + \lambda p(U),
\end{aligned}
\label{eqn:ICA}
\end{equation}
where $p(U)$ penalizes the non-independence of the columns of U.
Although there are many ways to optimize for statistical independence (or ``non-Gaussianity''), a common practical way of enforcing the independence is to use the log hyperbolic cosine penalty. The log hyperbolic cosine is a close approximation to the $\ell_1$ norm, Equation \ref{eqn:ICA} is closely approximated by 
\begin{equation}
\begin{aligned}
&\underset{U,V}{\operatorname{arg\,min}} & & \| X - UV^{\top} \|_2^2 + \lambda \sum_i \|U_i\|_1.
\end{aligned}
\end{equation}
It is worth noting that though we have cast SPCA and ICA in the same framework but still there are differences between the two in addition to what is described above. 

Firstly, the connection between PCA and ICA is tied to the assumption of using a log hyperbolic cosine penalty; if we were to use some other penalty then there might not be an obvious similarity between PCA and ICA, which makes sense as PCA considers only unto second order moments whereas ICA optimizes fourth order moments. 

Secondly, since PCA tries to find directions capturing decreasing variance (second order moment); there is a natural ordering to the components of PCA. However, the basis vectors found by standard ICA are not ranked in any order. So, above, we assume that the components of ICA are also ranked according to decreasing variance.

\subsection{Hybrid Decompositions}
Now, we can come up with hybrid decomposition schemes as the
following, which borrow ideas from SPCA and ICA.


\begin{equation}
\label{eq:eanat}
\underset{U,V}{\operatorname{arg\,min}} \; \| X - UV^{\top} \|_2^2 + \lambda_1 \sum_i \|U_i\|_1 + \lambda_2 \sum_i \|V_i\|_1,
\end{equation}
where $\lambda_1$ controls the contribution of the ICA sparsity component of the penalty and $\lambda_2$ controls the sparsity of the SPCA component of the penalty. 




%The statistical latent model has been widely used in the
%implementation of different matrix factorization algorithms. In this
%section, we will first build the most general  generative model and
%then discuss the degenerated situation where it goes to ICA, sPCA or
%NMF. 

\section{Eigenanatomy} 
The goal of eigenanatomy is to statistically learn the boundaries of and connections between brain regions by weighing both data and prior anatomical guidance.  In machine learning, such decompositions are termed ``parts-based representations'' because they transform unstructured data into interpretable pieces \cite{Lee1999,sparseNMF_hoyer}.  Recent work in machine learning points to the fact that exploiting problem-specific information can improve parts-based representations \cite{Guan2011,Cai2010,Hosoda2009}.  Uninformed, generic matrix decomposition methods, e.g. standard principal component analysis (PCA) or ICA, may be difficult to interpret because the solutions will produce vectors that are everywhere non-zero, i.e. involve the whole brain rather than its parts.  Sparse methods have sought to resolve this issue \cite{sparseNMF_hoyer,Witten2010,Friedman2010,Cherkassky2009,Friedman2008}.  However, these recent sparse multivariate methods are anatomically uninformed {and which may lead to unstable results} \cite{Xu2011}. Eigenanatomy component images, on the other hand, {enable prior knowledge to enhance solution stability} and are tied to a set of neuroanatomical coordinates that are {\em connected}, {\em smooth} and may also be defined by {\em non-negative} weights. 

% tester
% They are developed without the specific concerns of neuroimaging and anatomy taken into account.  That is, most existing implementations of machine learning methods ignore the neuroanatomical constraints necessary for biological plausibility and stable optimization of problems involving imaging.  Here, we propose a foundation for computing sparse eigenanatomy based on optimization of a penalized statistical criterion that incorporates anatomical priors.  
% Thus we implement an optimization-based, one-step computation to replace more ad hoc alternatives \cite{Tamnes2010,Wang2009,Walhovd2009} that typically rely on computing a $p \times p$ ($p=$number of voxels) correlation matrix followed by post-processing.  

% The class of eigenanatomy algorithms will approximate neuroimaging data with a set of sparse biologically plausible component images.   

The most critical eigenanatomy contribution is that each component image is tied to a set of neuroanatomical coordinates that are {\em connected}, {\em smooth} and are defined by {\em non-negative} weights.  Alhough this latter constraint can be relaxed, non-negativity improves our ability to interpret data by preventing weights from being both positive and negative within the same eigenanatomy component.  Also, non-negativity means that the projections of eigenanatomy into subject space are simply weighted averages of the input data (e.g. cortical thickness values) for each subject.   

The class of methods encompassing NMF, ICA, SPCA and singular value decomposition \cite{Sill2011,Lee2010b,Yeung2002} form the basis for the approach proposed here. In line with discussion in the previous section, the Eanat optimization problem is, 
\begin{equation}
\label{spca}
\min \|X-\hat{X} \|^2_F +  \text{eigenanatomy penalty terms},
\end{equation}
where we will define the penalty functions in an application-specific manner and the approximation matrix $\Xh$ will be derived from a collection of eigenanatomy components. The standard matrix decomposition techniques described in previous section-- SPCA, ICA, NMF and their hybrid extensions are the potential candidates for creating the approximation $\Xh$. 


%A variety of methods exist for formulating the approximating matrix $\Xh$ and also for the penalties associated with the approximation.  First, we denote each eigenanatomy component as $%{\bf e}_i$ where $i$ is ordered such that each eigenanatomy from ${\bf e}_1$ to ${\bf e}_m$ provides a decreasing contribution to the accuracy of the estimate matrix.  Note that %eigenanatomy is orthogonal, ${\bf e}_i \perp {\bf e}_j  \forall i \ne j$, and (by definition) {\em sparse}.  Typically, $\Xh$ will be derived from $m < n$ vectors where $n$ is the number %of subjects. 
%The classic approximation matrix based on SVD gives  
%$ \Xh =   \sum_{i=1}^m  \lambda_i  {\bf u}_i {\bf e}_i^T $
%where the ${\bf u}_i$ terms are the subject space projections of the eigenanatomy basis vectors and the $\lambda_i$ are the estimated eigenvalues (or singular values).  This formulation %can be used to decompose the $p \times p$ matrix efficiently, as in \cite{Witten2009b}.  Similar variants exist in NMF where the $\lambda$ are usually not written explicitly. 

We will now detail neurobiologically specific penalty terms.




\subsection{Eigenanatomy Penalty Terms} 
We will encode belief about brain connectivity via penalty terms based on recent {\em graph-based regularization} which may also be called {\em manifold regularization} \cite{Guan2011,Cai2010,Hosoda2009}.  We adopt the methods suggested for NMF in \cite{Cai2010,Guan2011} and tune them for anatomical data.  In this framework, one defines a weight function, $w_{kl}$, across edges where the $k^{th}$ and $l^{th}$ nodes are (for instance) cortical voxels and a higher weight indicates greater connectivity.  The weight function can then vary according to application:  {\bf for parcellating the cortex} the edge weight may be related to the expected membership within a Brodmann area;  {\bf for identifying long-range connections between cortical regions} the edge weight may be related to resting-state connectivity or white matter connectivity;  {\bf for clustering cortical regions} the edge weight may relate to the correlation, across subjects, of cortical thickness values.  This framework leads to the penalty term being represented as a (usually) sparse $p \times p$ matrix ${\bf L}$.  Then the generic eigenanatomy penalty term can be written as ${\bf e}_i^T {\bf L} {\bf e}_i$ where ${\bf L}$ is defined uniquely by the set of weights $w_{kl}$ and ${\bf e}_i$ are the different eigenanatomy components ordered such that each eigenanatomy from ${\bf e}_1$ to ${\bf e}_m$ provides a decreasing contribution to the accuracy of the estimate matrix.  Also, note that eigenanatomy is orthogonal, ${\bf e}_i \perp {\bf e}_j  \forall i \ne j$, and (by definition) {\em sparse}.

  For memory efficiency, the full matrix ${\bf L}$ will not be represented explicitly in the implementation.  {For instance, ${\bf L}$ may be formed by a voxel-wise measure of hippocampal connectivity based on white matter tractography where ${\bf L} = {\bf D}^T {\bf D}$ where ${\bf D}$ is $k \times p$ and encodes the connection probability from each of $k$ hippocampal voxels to the rest of the $p$ voxels in the brain.}

The Eanat optimization is exactly as in Equation~\ref{eq:eanat} where
we define the $i^{th}$ row of $V$ to be equal to ${\bf e}_i$.
The ${\bf e}_i$ are constrained as follows:
\begin{eqnarray}
S( {\bf e}_i ) = z^\star, \text{where $z^\star$ is a user constraint},\\ \notag
\forall_{i \ne j} \langle {\bf e}_j ,  {\bf e}_i \rangle = 0~~\&\&~~{\bf e}_i \succeq 0,\\ \notag
\nabla^2 {\bf e}_i < \kappa, \text{ i.e. smoothness}.
\end{eqnarray}
Comments on each constraint follow.

\noindent{\em Inner-product:} Standard PCA uses orthogonality.  Eanat
enforces that the innerproduct between the Eanat componenents is
zero.  
  
\noindent{\em Non-negativity:}   Non-negativity means that the projections of eigenanatomy into subject space are simply weighted averages of the input data (e.g. cortical thickness values) for each subject.   Although this constraint can be relaxed, non-negativity improves our ability to interpret data by preventing weights from being both positive and negative within the same eigenanatomy component. As such, one may compute effect sizes and interpret statistics directly, for example, ``reductions in posterior cingulate cortical thickness reduce performance on memory-related psychometrics.''  Sparseness can also be implemented by adding a $l_1$ penalty term to the objective function as in numerous machine learning methods \cite{sparseNMF_hoyer,sparseNMF_kim,sparseNMF_heiler,sparsePCA_zou,sparsePCA_jordan,sparsePCA_journee,Gandy2010,Chennubhotla2001,Lee2011}.  %Within eigenanatomy, there is a novel interaction between non-negativity and sparseness that is natural for the brain.  That is, 
%Sparseness, non-negativity and orthogonality are related by the following fact:  ${\bf e}_i \perp {\bf e}_j$ if entries that are zero in ${\bf e}_i$ are non-zero in ${\bf e}_j$ and vice-versa.  % Thus, one may not always require a $l_1$ penalty if there are sufficient (anatomically) orthogonal eigenanatomy components.  
To our knowledge, directly exploring the interaction between sparseness, orthogonality and non-negativity for automated parcellation of the brain is novel.  However, this penalty set alone is insufficient to guarantee anatomically reasonable results.


\noindent{\em Sparsity \& Smoothness:} We enforce sparsity and
smoothness of the Eanat components by a projection method.  This
involves a nonlinear optimization of the following constraints:
\begin{eqnarray}
\text{find $\gamma^\star$ s.t.}  \notag \\
\text{$e_i$ is clustered}  \notag \\
\text{$e_i$ is smooth}  \notag \\
\text{$e_i$ is sparse} 
\end{eqnarray}
The optimal $\gamma^\star$ is identified by a binary search. 
The algorithm thresholds $e_i$ by $\gamma$, clusters $e_i$ and then
smooths $e_i$ then computes sparsity constraint i.e. $non-zero( e_i
)=z$.   The search is continued until $z=z^\star$.  Finally, the
constraints are again  applied starting with the $\ell_1$ threshold
$\gamma^\star$.  

\subsection{Eigenanatomy Implementation} 
\noindent{\bf Eigenanatomy optimization algorithm.}  We will implement our own Arnoldi iteration (similar to \cite{Gandy2010,sparsePCA_journee}) and the gradient-based methods proposed in \cite{Cai2010} and also in \cite{Lee1999}.  We will develop the general framework and matrix representation above starting with the classic Arnoldi iteration method.  This technique is appropriate for large-scale problems and fits within the standard alternating minimization approach used by NMF and SPCA methods \cite{Gandy2010}.  

One could optimize the eanat objective in Equation~\ref{spca} using an iterative approach like power iteration. However, since the enact objective is non-convex we risk the chance of getting stuck in a bad local optima. Specifically, with that in mind, we propose a novel optimization algorithm for the eanat objective; particularly, we ``deflate'' our data matrix {\bf X} (factoring out the effect of other eigenvectors) between computations of different eigenvectors, which leads to better solutions~\citep{mackey08}. The deflation based optimization entails performing an additional ordinary least squares regression (OLS) step and can be motivated as follows.


We know that the best rank `k' reconstruction of a matrix  i.e. $\argmin_{\hat{\bf X}} \|\bf X~-~\hat{X} \|_F^2$, is provided by its first `k' eigenvectors~\citep{eckart} i.e. $\hat{\bf X}=\sum_{i=1}^k d_k{u}_k{v}_k^{\top}$.

Hence, the best rank-1 approximation of {\bf X}, i.e. the $n\times 1$ and $p \times 1$ vectors ${\tilde{u}}$, ${\tilde{v}}$ such that,
\begin{equation}
\label{ppca1}
{\tilde{u}^*, \tilde{v}^*}= min_{{\tilde{u}}, {\tilde{v}}} \|{{\bf X}- \tilde{u}\tilde{v}^{\top}}\|_F^2
\end{equation}
is given by the SVD solution-- ${\tilde{u}=u_1}$ and ${\tilde{v}}=d_1{v_1}$, where ${u_1 }$, ${v_1}$ and $d_1$ are the first left and right eigenvectors and the eigenvalue, respectively, of the {\bf X} matrix.   

Proceeding this way, $d_2{u}_2{v}_2^{\top}$ provide the best rank-1 approximation of the ``deflated'' matrix ${{\bf X}- d_1{u}_1{v}_1^{\top}}$ and so on. 


As pointed by~\citep{shen},  with ${\tilde{v}}$ fixed, the above optimization over  ${\tilde{u}}$ is equivalent to a least squares regression of {\bf X} on  ${\tilde{v}}$. Once we have ${\tilde{u}}$, we solve for ${\tilde{v}}$ by performing power iteration followed by sparse ``soft thresholded'' projection with the following objective

\begin{equation}
\label{spca1}
{v_i^*}= \argmax_{{v_i},{\|v_i\|}=1,  v_i^{\top}v_j=0, i\neq j} {v_i}^{\top}\left(X_{\backslash i}^{\top}X_{\backslash i} \right){v_i} +  \lambda_i\|{v_i}\|_1 
\end{equation}
where ${\bf X}_{\backslash i}  \triangleq {\bf X}- \sum_{j=1, j\neq i}^{k}{\tilde{u}_j\tilde{v}_j^{\top}}$ is the ``deflated'' {\bf X} matrix.



Our proposed algorithm for eanat iterates between Equations~\ref{spca} and \ref{spca1} i.e. with ${\tilde{v}}$ fixed, performing least squares regression to find  ${\tilde{u}}$ and then holding  ${\tilde{u}}$ fixed and solving to get the sparse eigenvectors ${\tilde{v}}$ via conjugate gradient. 


The sparseness, smoothness and non-negativity are enforced as discussed in the previous section.

 The details of our algorithm can be found in Algorithms~\ref{algo1},~\ref{algo2}. Note that the OLS regression step is just required to compute $\tilde{u}$ which is required for deflation of the data matrix.

\begin{algorithm}[htbp]
\small \caption{\bf  Eigenanatomy: eanat (Main Algorithm)}
\label{algo1}
\KwIn { {\bf X}, $\lambda$ }
Standardize the data matrix {\bf X}: Mean center it and scale to unit variance\;
Initialize the eigenvectors randomly ${\bf V \gets \mathcal{N}(0,1)}$\;
${\bf U} \gets$ {\texttt {ReconOpt}}(${\bf X}, {\bf V}$)\;
\While {${\Delta {\bf \|V\|}} \leq \epsilon$ }{	
\For{i=1 to k}{
	${\bf X_{\backslash i}}  \gets {\bf X}- \sum_{j=1, i\neq j}^{k}{u_jv_j^{\top}}$\Comment{Deflate {\bf X}}\;
	${v_i} \gets$ {{\texttt {SP}}}(${\bf X}_{\backslash i}, {\bf V},  \lambda,i$)\;
	${\bf U} \gets$ {\texttt {ReconOpt}}(${\bf X}, {\bf V}$) \Comment{Where {\bf V} is the matrix with only its $i^{th}$ column updated. Other columns are the same as previous iteration}\;
	}
	}
\KwOut {${\bf V}$}
\end{algorithm}
\begin{algorithm}[htbp]
\label{algo2}
\DontPrintSemicolon
\SetKwFunction{KwFun}{ReconOpt} \;
\KwFun{${\bf X}$, ${\bf V}$}\Comment{Optimize Reconstruction error for finding $u_i$}\;
{
\hspace{0.5cm}	${\bf U}   \gets ({\bf XX^{\top}})^{-1}{\bf XV}$ \Comment{Performs Ordinary Least Squares Regression.}\;
\hspace{0.5cm} return {\bf U}
}

\vspace{1cm}

\SetKwFunction{KwFun}{SCGP}\;
\KwFun{${\bf X}$, ${\bf V}$, $\lambda$,i } \Comment{Sparse Conjugate Gradient Projection for finding $v_i$}\;
{
\hspace{0.5cm}$k \gets 1$\;
\hspace{0.5cm}${c}^k \gets {\bf V}_{:,i}$ \Comment{${\bf V}_{:,i}$ contains the $i^{th}$ eigenvector.}\;
\hspace{0.5cm}${c}^k \gets$ {\texttt S}(${c}^k$,$\gamma$) \Comment{Soft-Max Thresholding.}\;
\hspace{0.5cm}${g}^{k-1} \gets$ 1 \Comment{Initialize Gradient.}\;

\While {${\Delta c} \leq \epsilon$ }{

$ g^{k} \gets ({\bf X^{\top}X}){c}^{k}$  \Comment{Gradient.}\;
$\gamma \gets  \frac{g^{k}\cdot g^{k}}{g^{k-1} \cdot g^{k-1}}$ \Comment{Conjugate Gradient.} \;
$ d^{k} \gets g^{k}  + \gamma\cdot d^{k-1}$ \;
${ c}^{k+1} \gets  {c}^k + d^{k}$  \;
${c}^{k+1} \gets$ {\texttt {Orthogonalize}}(${c}^{k+1}$,{ $\bf V_{:,\backslash i}$}) \Comment{Orthogonalize w.r.t. all the other $k-1$ eigenvectors.}\;
 ${c}^{k+1} \gets$ {\texttt S}(${c}^{k+1}$,$\gamma$) \;
${ c}^{k+1} \gets \frac{{c}^{k+1}}{\| {c}^{k+1}\|}$ \Comment{Normalize}\;
 $k \gets k+1$\;
}
\hspace{0.5cm} return ${c^{k+1}}$
}
\small \caption{\bf Eigenanatomy: eanat (Sub Algorithm)}
\end{algorithm}


\section{Results}

% \subsection{Model Comparison between sPCA, ICA and NMF}
\subsection{Outline of results}
\begin{enumerate}
  \item[Figure 1] Graphical/cartoon outline of parts-based representations for neuroimaging   
  \item[Table 1] Tabular description of the various decompositions and what they penalize
  \item[Figure 2]  
  \item[Figure 3]
\end{enumerate}

ANTsR illustration of eigenanatomy (eanat) in a p-value and a prediction study.  We also illustrate visualization and descriptive statistics with eanat.



\section{Read in data}

We read in the population data, template, mask, etc.

<<readdata,eval=FALSE>>=
tem<-antsImageRead('data/template_brain.nii.gz',3)
bm<-antsImageRead('data/brain_mask.nii.gz',3)
mask <- getMask("data/maskgb.nii.gz")
mat <- as.matrix( antsImageRead( 'data/thicknesspt25.mha' ,2 ) )
subjin<-read.csv('data/subjid.csv')[1:nrow(mat),]
ncon<-48
nmci<-nrow(mat)-ncon
dx<-c(rep(0,ncon),rep(1,nmci))
mydata<-data.frame( dx=dx, subjin = subjin , mat=mat )
mydata_sub <- mydata[sample(1:nrow(mat), 50,   replace=FALSE),]
basedir<-'eanat_processing'
@

\section{Reduce dimensionality}

Next we do the eanat decomposition.

<<eigenanatomy,eval=FALSE>>=

####################
its<-1           # max iterations in optimization
nvecs<-11      # components to split data into ...
usp<-( -1 )     # ICA style if abs( usp ) < 1 or nmf style if usp & vsp> 0
vsp<-0.02        # components occupy 10% of the mask
ii<-0              # index for this study 
cl<-50         # ignore small clusters 
sm<-1         # smoothing
basedire<-paste(basedir,ii,'/',sep='')
dir.create(file.path("./", basedire), showWarnings = FALSE)
# if ( ! exists('mydecom') )
  mydecom<-sparseDecom( as.matrix(mydata[,3:ncol(mydata_sub)]), mask,  sparseness=vsp, nvecs=nvecs, its=its, cthresh=cl, statdir=basedire,smooth=sm, z=usp)
#  mydecom<-sparseDecom( as.matrix(mydata_sub[,3:ncol(mydata_sub)]), mask,  sparseness=vsp, nvecs=nvecs, its=its, cthresh=cl, statdir=basedire,smooth=0.0, z=usp)
decom1<-mydecom$umatrix
decom2<-imageListToMatrix( mydecom$eigenanatomyimages, mask)
myproj<-mat %*% t( decom2 )
colnames(myproj)<-paste("V",1:ncol(myproj),sep='')
eigParcellation<-eigSeg( mask , mydecom$eigenanatomyimages )
dir.create('tempoutput')
antsImageWrite(eigParcellation,'tempoutput/eigseg.nii.gz')

@ 


<<egraph,eval=FALSE>>=

# first fuse with high density to get few predictors , might have to alter density to get what you want
graphdensity <- 0.65 
fused<-joinEigenanatomy( mat, mask,  mydecom$eigenanatomyimages, graphdensity )

@

\section{Eanat prediction}

Use the components to cross-validate a prediction model.

<<prediction,eval=FALSE>>=
#
### use the networks in prediction                   
#
subj<-data.frame( cbind( subjin, dx, fused$fusedproj ) )
myform<-paste( "dx~", paste( colnames( fused$fusedproj ) , collapse='+' ) )
mdl<-glm( as.formula(myform), data=subj, family="binomial")
dd<-0
for ( i in 1:100 ) dd<-dd+cv.glm(subj, mdl,K=10)$delta[1]*0.01
print(paste("prediction % misclassification" , dd * 100 ) )

@ 

\section{Eanat morphometry}

Use the components in a simple group comparison.

<<pvalue,eval=FALSE>>=
#
###     perform a t-test on the networks    
#
pv<-rep(NA, ncol(myproj) ) 
for ( i in 1:ncol(myproj) )
  {
  pv[i]<-t.test( myproj[1:ncon,i],myproj[(ncon+1):nrow(myproj),i] )$p.value
  print( paste("p-value",i,pv[i]   ))
  }
qv<-(p.adjust(pv,method='bonferroni'))
print(qv)

@ 



<<vbm,eval=FALSE>>=
#
###     perform a t-test on the voxels                                    
###     contrast bonferroni & FDR correction                          
#
vpv<-rep(NA, ncol(mat) ) 
for ( i in 1:ncol(mat) )
  {
  vpv[i]<-t.test( mat[1:ncon,i],mat[(ncon+1):nrow(mat),i] )$p.value
 }
vqv<-(p.adjust(vpv,method='bonferroni'))
print( min( vqv ) ) 
vqvi<-antsImageClone( mask )
vqvi[ mask > 0 ]<-1-vqv
antsImageWrite(vqvi,'vqvi_bonf.nii.gz')
vqv<-(p.adjust(vpv,method='BH'))
print( min( vqv ) ) 
vqvi<-antsImageClone( mask )
vqvi[ mask > 0 ]<-1-vqv
antsImageWrite(vqvi,'vqvi_BH.nii.gz')
@ 



\section{Eanat interpretation}

Describe the components in terms of traditional coordinates. Also render the results. 

<<figsetup,echo=TRUE,results='hide',warning=FALSE,message=FALSE, echo=FALSE,eval=FALSE>>=
#
### describe the significant networks 
#
if ( ! exists("mymni") ) {
  mymni<-list( antsImageRead(getANTsRData('mni'),3), 
            antsImageRead(getANTsRData('mnib'),3), 
            antsImageRead(getANTsRData('mnia'),3) )
  }

mysig<-order(qv)
brain<-renderSurfaceFunction( surfimg =list( bm ) , alphasurf=0.1 , smoothsval = 1.5  )
id<-par3d("userMatrix")
rid<-rotate3d( id , -pi/2, 1, 0, 0 )
rid2<-rotate3d( id , pi/2, 0, 0, 1 )
rid3<-rotate3d( id , -pi/2, 0, 0, 1 )
par3d(userMatrix = id ) 
@

This is the visual and anatomical description of the most significant network.
<<describeit,echo=FALSE,results='hide',warning=FALSE,message=FALSE, echo=FALSE,dev='png', fig.width=4, fig.height=4, out.width='.5\\linewidth',eval=FALSE>>=
opts_chunk$set(fig.path='figure/Eigenanatomy1-', fig.align='center', fig.show='asis',fig.keep='all')
dir.create('figure')
i<-mysig[1]
mytem<-lappend( list(tem), mydecom$eigenanatomyimages[[ i ]] )
if ( cl < 100 ) cl <- 100
signifnetworkdescriptor<-getMultivariateTemplateCoordinates(  mytem, mymni , convertToTal = TRUE , pvals = pv[ i ] , threshparam = 0.5, clustparam = cl, identifier = "_Sig1", outprefix="./PP" )
f<-mydecom$eigenanatomyimages[[ i ]]
cnt<-getCentroids( f, clustparam = cl, threshparam = 0.5 )
brain<-renderSurfaceFunction( surfimg =list( bm ) , alphasurf=0.1 ,smoothsval = 1.5 , smoothfval = 1.0, funcimg=list(cnt$clustimg) , alphafunc=0.2 )
plotBasicNetwork( centroids =  cnt$centroids , brain )
make3ViewPNG(  rid, id, rid2,  paste('figure/network1',sep='') )
par3d(userMatrix = id ) 
@ 
\begin{figure}[h]
  \centering
    \includegraphics[width=1.0\textwidth]{figure/network1.png}
  \caption{A significant network.}
\end{figure}

The anatomy of this component and Talairach coordinates.
<<tableTest,results='asis',echo=FALSE,eval=FALSE>>=
ntwkd<-signifnetworkdescriptor$networks[,c(1,2,3,4,7,8,9)]
test <- data.frame( ntwkd )
print(xtable(test))
@

The decomposition extracts regions that covary.  So, let's look at the anatomical correlations within this network.  

<<netcorr,results='hide',echo=FALSE,warning=FALSE,message=FALSE,eval=FALSE>>=
nnodes<-nrow( cnt$centroids )
clustimgs<-image2ClusterImages( cnt$clustimg , minThresh =1, maxThresh = nnodes )
clustmat<-imageListToMatrix( clustimgs, mask )
means<-apply(clustmat,MARGIN=1,FUN=mean)
clustmat<-clustmat/means
clustmat<-mat %*% t(clustmat)
colnames(clustmat)<-test$AAL[2:length(test$AAL)]
library(pheatmap)
png('figure/within_network_correlation.png')
pheatmap( cor( clustmat ) ,symm=T )
dev.off()
@
\begin{figure}[h]
  \centering
    \includegraphics[width=0.95\textwidth]{figure/within_network_correlation.png}
  \caption{Correlation between values at the nodes of the most significant network.}
\end{figure}


How do the individual networks vary across the whole population?

<<spider,results='hide',echo=FALSE,warning=FALSE,message=FALSE,eval=FALSE>>=
row.names(clustmat)<-subjin
con_avg<-apply( clustmat[1:ncon,],MARGIN=2,FUN=mean)
mci_avg<-apply( clustmat[(ncon+1):nrow(myproj),],MARGIN=2,FUN=mean)
clustmat<-rbind( clustmat, con_avg)
clustmat<-rbind( clustmat, mci_avg)
pdf('figure/spider.pdf')
stars( clustmat,main = "Network Thickness: Control vs MCI", flip.labels = FALSE, radius = TRUE, key.loc=c(20,1) )
# stars(clustmat, len = 0.6, key.loc = c(20,2), main = "Network Thickness: Control vs MCI", draw.segments = TRUE,   frame.plot = TRUE,  flip.labels = FALSE, cex = .7)
dev.off()
@
\begin{figure}[h]
  \centering
    \includegraphics[width=0.95\textwidth]{figure/spider.pdf}
  \caption{{\bf Spider plots:} We show the thickness at each node of the network in every individual as well as the average for the controls and MCI subjects.}
\end{figure}



Now let's look at the rest of the significant networks.
<<describeit2,rgl=FALSE,echo=FALSE,results='hide',warning=FALSE,message=FALSE,dev='png', fig.width=4, fig.height=4, out.width='.5\\linewidth',eval=FALSE>>=
opts_chunk$set(fig.path='figure/Eigenanatomy1-', fig.align='center', fig.show='asis',fig.keep='all')
ct<-2
for ( i in mysig[2:4] )
  {
  mytem<-lappend( list(tem), mydecom$eigenanatomyimages[[ i ]] )
  signifnetworkdescriptor<-getMultivariateTemplateCoordinates(  mytem, mymni , convertToTal = TRUE , pvals = pv[ i ] , threshparam = 0.5, clustparam = cl,  identifier = "_Sig2", outprefix="./Z" )
  f<-mydecom$eigenanatomyimages[[ i ]]
  cnt<-getCentroids( f , clustparam = cl, threshparam = 0.5  )
  brain<-renderSurfaceFunction( surfimg =list( bm ) , alphasurf=0.1 ,smoothsval = 1.5 , smoothfval = 1.0, funcimg=list(cnt$clustimg) , alphafunc=0.2 )
  plotBasicNetwork( centroids =  cnt$centroids , brain )
  par3d(userMatrix = id ) 
  make3ViewPNG(  rid, id, rid2,  paste('figure/network',ct,sep='') )
  par3d(userMatrix = id ) 
  ct<-ct+1
  }
 @
 
 \begin{figure}[h]
  \centering
    \includegraphics[width=1.0\textwidth]{figure/network2.png}
  \caption{The 2nd significant network with $q$ \Sexpr{qv[mysig[2]]}.}
\end{figure}

 
\begin{figure}[h]
  \centering
    \includegraphics[width=1.0\textwidth]{figure/network3.png}
  \caption{The 3rd significant network with $q$ \Sexpr{qv[mysig[3]]}.}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=1.0\textwidth]{figure/network4.png}
  \caption{The 4th significant network with $q$ \Sexpr{qv[mysig[4]]}.}
\end{figure}



The anatomy of this component and Talairach coordinates.
<<tableTest2,results='asis',echo=FALSE,warning=FALSE,message=FALSE,eval=FALSE>>=
i<-mysig[2]
mytem<-lappend( list(tem), mydecom$eigenanatomyimages[[ i ]] )
signifnetworkdescriptor<-getMultivariateTemplateCoordinates(  mytem, mymni , convertToTal = TRUE , pvals = pv[ i ] , threshparam = 0.5, clustparam = cl,  identifier = "_Sig2", outprefix="./Z" )
ntwkd<-signifnetworkdescriptor$networks[,c(1,2,3,4,7,8,9)]
test <- data.frame( ntwkd )
print(xtable(test))
@



<<compareDecom, warning=FALSE, results='hide',eval=FALSE>>=
its<-5
nvecs<-6 # 0
studynames<-c(  "NMF", "ICA", "sPCA", "v+sPCA", "sPICA","v+sPICA", "PCA","fastICA" )
u<-1/20
v<-1/2
mp<-data.frame( studynames=studynames, 
                uSparseness=c(u, -1*u, -1, -1, -1*u, -1*u, NA, NA), 
                vSparseness=c( v, -1, -1*v, v, -1*v, v, NA, NA))
myresults<-rep(NA,nrow(mp))
myqct<-rep(NA,nrow(mp))
myk<-rep(NA,nrow(mp))
mymi<-rep(NA,nrow(mp))
mycorct<-rep(NA,nrow(mp))
mypred<-rep(NA,nrow(mp))
resultlist<-list()
resultlist2<-list()

for ( ii in c(1:nrow(mp)) ) {
  basedire<-paste(basedir,ii,'/',sep='')
  dir.create(file.path("./", basedire), showWarnings = FALSE)
  print(paste("Params: u ",mp$uSparseness[ii]," v ",mp$vSparseness[ii]))
  if ( ii < (nrow(mp)-1))
    mydecom<-sparseDecom( (mat), mask,  sparseness=mp$vSparseness[ii],
                          nvecs=nvecs, its=its, cthresh=5550, 
                          statdir=basedire,smooth=0, z=mp$uSparseness[ii])
  seg<-eigSeg(mask, mydecom$eigenanatomyimages )
  antsImageWrite(seg,paste(basedire,'eig_seg_',ii,'x.nii.gz',sep=''))
  dx<-c(rep(0,ncon),rep(1,nmci))
  decom1<-mydecom$umatrix
  decom2<-mydecom$umatrix

  if ( TRUE ) {
  ct<-1
  for ( x in mydecom$eigenanatomyimages ) {
    if ( ct < (ncol(decom2)+1) ) {
      vec<-x[ mask == 1 ]
      vec<-abs( vec )
      vec<-vec/sum(vec)
      wavg<-( mat %*% vec )
      decom2[ ,ct]<-( wavg )
      ct<-ct+1
      }
  }
  }
 if ( ii == (nrow(mp)) )
    {
    print("begin fastICA")
    myica<-fastICA( t(mat) ,nvecs)
    decom2<-mat %*% myica$S # t(decom2$A)
    decom1<-t(myica$A)
    }
  if ( ii == (nrow(mp)-1) )
    {
    pca<-princomp( t(mat) )
    decom2 <- mat %*% pca$scores
    decom1 <- t( as.matrix( pca$loadings[1:nvecs,]) )
    }
  pvals<-rep(NA,ncol(decom2))
  ttvals<-rep(NA,ncol(decom2))
  for ( x in 1:ncol(decom2) )
    {
    tt<-( t.test( decom2[1:ncon, x ] , decom2[ (ncon+1):(ncon+nmci), x ] ) )
    pvals[x]<-tt$p.value
    ttvals[x]<-tt$statistic
    }
  qvals<-(p.adjust(pvals,method="BH"))
  print(qvals)
  print("tvals")
  print(ttvals)
  myresults[ii]<-sum(qvals<0.050001)
  mycor<-cor( decom2 )
  mycorct[ii]<-mean( abs( mycor[  mycor < 1 ] ) )
  myk[ii]<-sum(abs(c(moments::kurtosis(decom1))))
  muti<-0 ; mict<-0
  disc<-discretize(decom1,nbins=16)
  for ( x in 1:nvecs ) for ( y in 1:nvecs )
    if ( x != y )
      {
      muti<-muti+mutinformation( disc[,x], disc[,y],method="emp")
      mict<-mict+1
      }
  print( muti/mict )
  mymi[ii]<-muti/mict
  colnames(decom2)<-paste("V",1:ncol(decom2),sep='')
  subj<-as.data.frame(cbind(subjin,dx,decom2))
  myform<-paste("dx~",paste(colnames(decom2),collapse='+'))
  mdl<-glm( as.formula(myform), data=subj, family="binomial")
  print("begin cv")
  dd<-0
  for ( i in 1:100 ) dd<-dd+cv.glm(subj, mdl,K=10)$delta[1]*0.01
  mypred[ii]<-dd[1]
  mydf<-data.frame(meanCorrelation=mycorct,
                   kurtosis=myk,
                   mutualinformation=mymi,
                   qValLtPoint05=myresults,
                   prediction=mypred)
  mydf<-cbind(mp,mydf)
  print(mydf)
  resultlist <-lappend( resultlist, mydecom  )
  resultlist2<-lappend( resultlist, decom2 )
  library(xtable)
  xtbl<-xtable(mydf)
  print(xtbl,floating=FALSE, file='results.tex', booktabs=T)
  write.csv(mydf,'quick_results2b.csv')
}

@

\input{results.tex}
Now we are done.  The results are: 
\begin{enumerate}
\item An anatomically interpretable decomposition.
\item Its use in a prediction study for mild MCI---results are comparable to state of the art methods.
\item Application to standard morphometry to reveal network level differences in cortical thickness patterns.
\end{enumerate}
Enjoy! 

\bibliographystyle{IEEEtran}
\bibliography{icapca}
\end{document}
